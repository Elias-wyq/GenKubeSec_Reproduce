import json
import pandas as pd
import os
from collections import defaultdict

# --- 1. é…ç½®è·¯å¾„ ---
MAPPING_FILE = "policies_with_remediation.csv"
INPUT_FILES = {
    "checkov": "/home/wyq/GenKubeSec_Reproduce/kcfs_results/RB_tool_results/checkov_full_results.jsonl",
    "kubelinter": "/home/wyq/GenKubeSec_Reproduce/kcfs_results/RB_tool_results/kubelinter_full_results.jsonl",
    "terrascan": "/home/wyq/GenKubeSec_Reproduce/kcfs_results/RB_tool_results/terrascan_full_results.jsonl"
}
OUTPUT_FILE = "/home/wyq/kcfs_results/final_labels.jsonl"

def normalize_text(text):
    """æ–‡æœ¬æ ‡å‡†åŒ–ï¼šå»é™¤å‰åç©ºæ ¼"""
    if not isinstance(text, str):
        return ""
    return text.strip()

def load_mapping(filepath):
    """åŠ è½½ CSV æ˜ å°„è¡¨"""
    if not os.path.exists(filepath):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° CSV æ–‡ä»¶ {filepath}")
        return None, None, None

    df = pd.read_csv(filepath, dtype=str).fillna("")
    
    ckv_map = {}
    ter_map = {}
    kbl_map_remediation = {}

    for _, row in df.iterrows():
        umi_id = row['ID']
        
        # Checkov åŒ¹é…å­—å…¸
        c_policy = normalize_text(row.get('Checkov_Policy', ''))
        if c_policy: ckv_map[c_policy] = umi_id

        # Terrascan åŒ¹é…å­—å…¸
        t_policy = normalize_text(row.get('Terrascan_Policy', ''))
        if t_policy: ter_map[t_policy] = umi_id

        # KubeLinter (Remediation) åŒ¹é…å­—å…¸
        rem = normalize_text(row.get('Remediation', ''))
        if rem: kbl_map_remediation[rem] = umi_id
    
    print(f"âœ… æ˜ å°„è¡¨åŠ è½½å®Œæˆã€‚Checkov: {len(ckv_map)}, Terrascan: {len(ter_map)}, KubeLinter: {len(kbl_map_remediation)}")
    return ckv_map, ter_map, kbl_map_remediation

def process_file(filepath, tool_name, mapping, global_data):
    """
    è¯»å–å•ä¸ªæ–‡ä»¶ï¼Œè§£æå¹¶æ›´æ–°åˆ°å…¨å±€å­—å…¸ global_data ä¸­
    """
    if not os.path.exists(filepath):
        print(f"âš ï¸ è·³è¿‡: æ–‡ä»¶ä¸å­˜åœ¨ {filepath}")
        return

    print(f"ğŸ“– æ­£åœ¨è¯»å– {tool_name} ç»“æœ...")
    count = 0
    matched_count = 0
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            
            try:
                entry = json.loads(line)
                filename = entry.get('filename')
                if not filename: continue
                
                count += 1
                
                # 1. æ”¶é›† Resource Kind å€™é€‰ (ç”¨äºåç»­è¡¥å…¨ Unknown)
                # åªè¦è¯¥æ–‡ä»¶åœ¨ä»»æ„å·¥å…·ä¸­è¯†åˆ«å‡ºäº†æœ‰æ•ˆçš„ Kindï¼Œå°±å­˜ä¸‹æ¥
                for err in entry.get('errors', []):
                    k = err.get('kind', 'Unknown')
                    if k and k != "Unknown":
                        global_data[filename]["kinds"].append(k)
                
                # 2. åŒ¹é…é”™è¯¯è§„åˆ™å¹¶è®°å½• ID
                for err in entry.get('errors', []):
                    matched_id = None
                    
                    if tool_name == "checkov":
                        # Checkov: ç”¨ check_name åŒ¹é…
                        key = normalize_text(err.get('check_name', ''))
                        if key in mapping: matched_id = mapping[key]
                            
                    elif tool_name == "terrascan":
                        # Terrascan: ç”¨ description åŒ¹é…
                        key = normalize_text(err.get('description', ''))
                        if key in mapping: matched_id = mapping[key]
                            
                    elif tool_name == "kubelinter":
                        # KubeLinter: ç”¨ remediation åŒ¹é…
                        key = normalize_text(err.get('remediation', ''))
                        if key in mapping: matched_id = mapping[key]
                    
                    if matched_id:
                        global_data[filename]["umi_ids"].add(matched_id)
                        matched_count += 1
                        
            except json.JSONDecodeError:
                pass
    
    print(f"   â””â”€ å·²å¤„ç† {count} ä¸ªæ–‡ä»¶è®°å½•ï¼ŒæˆåŠŸåŒ¹é… {matched_count} ä¸ªé”™è¯¯é¡¹ã€‚")

def main():
    # 1. åŠ è½½ CSV æ˜ å°„
    ckv_map, ter_map, kbl_map_rem = load_mapping(MAPPING_FILE)
    if not ckv_map: return

    # 2. åˆå§‹åŒ–å…¨å±€æ•°æ®å®¹å™¨
    # ç»“æ„: { "file_1.yaml": { "kinds": ["Service", ...], "umi_ids": set("1", "52") } }
    # ä½¿ç”¨ defaultdict è‡ªåŠ¨å¤„ç†æ–°æ–‡ä»¶
    global_data = defaultdict(lambda: {"kinds": [], "umi_ids": set()})

    print("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®åˆ°å†…å­˜ (å­—å…¸æ¨¡å¼)...")

    # 3. ä¾æ¬¡å¤„ç†ä¸‰ä¸ªæ–‡ä»¶ (é¡ºåºä¸é‡è¦ï¼Œå› ä¸ºæ˜¯æŒ‰ filename èšåˆ)
    process_file(INPUT_FILES["checkov"], "checkov", ckv_map, global_data)
    process_file(INPUT_FILES["terrascan"], "terrascan", ter_map, global_data)
    process_file(INPUT_FILES["kubelinter"], "kubelinter", kbl_map_rem, global_data)

    print(f"ğŸ’¾ å†…å­˜åŠ è½½å®Œæ¯•ï¼Œå…±æ¶‰åŠ {len(global_data)} ä¸ªå”¯ä¸€æ–‡ä»¶ã€‚æ­£åœ¨å†™å…¥ç»“æœ...")

    # 4. ç”Ÿæˆæœ€ç»ˆç»“æœ
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        for filename, data in global_data.items():
            
            # ç¡®å®šæœ€ä½³ Kind (æŠ•ç¥¨æœºåˆ¶)
            best_kind = "Unknown"
            if data["kinds"]:
                # ç®€å•å–ç¬¬ä¸€ä¸ªé Unknown çš„ï¼Œæˆ–è€…ä½ å¯ä»¥å†™æ›´å¤æ‚çš„ç»Ÿè®¡é€»è¾‘
                # å› ä¸ºé€šå¸¸ä¸€ä¸ªæ–‡ä»¶çš„ Kind æ˜¯å”¯ä¸€çš„
                best_kind = data["kinds"][0]
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½• UMI IDï¼Œåˆ™è·³è¿‡ (æˆ–è€…è§†æƒ…å†µä¿ç•™ç©ºåˆ—è¡¨)
            if not data["umi_ids"]:
                continue

            # ç”Ÿæˆæ ‡ç­¾: Kind+ID
            final_labels = [f"{best_kind}+{uid}" for uid in data["umi_ids"]]
            
            record = {
                "filename": filename,
                "misconfig_labels": sorted(list(set(final_labels))),
                "error_count": len(final_labels)
            }
            f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()