{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c93961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: å¯¼å…¥åº“ä¸è·¯å¾„é…ç½®\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# è®¾ç½® Hugging Face å…¨å±€ç¼“å­˜ç›®å½•\n",
    "os.environ[\"HF_HOME\"] = \"/ssd_2t_1/wyq_workspace/hf_cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- é…ç½® ---\n",
    "# Mistral æ¨¡å‹è·¯å¾„ (å¦‚æœä½ å·²ç»ä¸‹è½½åˆ°æœ¬åœ°ï¼Œæ”¹æˆæœ¬åœ°è·¯å¾„ï¼›å¦åˆ™ä» HF æ‹‰å–)\n",
    "# è®ºæ–‡ä½¿ç”¨çš„æ˜¯ v0.2 Instruct ç‰ˆæœ¬\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "\n",
    "# ä½ çš„ UMI æ˜ å°„æ–‡ä»¶ (Stage 1 äº§å‡º)\n",
    "# æ ¼å¼åº”è¯¥æ˜¯ CSV: ID, Description, ...\n",
    "UMI_CSV_PATH = \"/home/wyq/GenKubeSec_Reproduce/policies_with_remediation.csv\"\n",
    "\n",
    "# ä½ çš„æ£€æµ‹æ¨¡å‹è¾“å‡ºç»“æœ (Stage 2 äº§å‡ºï¼Œç”¨äºæµ‹è¯•)\n",
    "# å‡è®¾æ˜¯ä¸€ä¸ª jsonl æ–‡ä»¶ï¼ŒåŒ…å« filename, content, predicted_labels\n",
    "DETECTION_RESULT_PATH = \"/home/wyq/kcfs_results/genkubesec_dataset/test_results_sample.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: åŠ è½½ UMI æ˜ å°„è¡¨\n",
    "def load_umi_mapping(csv_path):\n",
    "    \"\"\"\n",
    "    å°† CSV åŠ è½½ä¸ºå­—å…¸: { \"52\": \"Image tag ':latest' used\", ... }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(UMI_CSV_PATH, dtype=str).fillna(\"\")\n",
    "        # å‡è®¾ CSV æœ‰ 'ID' å’Œ 'Checkov_Policy' (æˆ–è€…å…¶ä»–æè¿°åˆ—)\n",
    "        # è¿™é‡Œæˆ‘ä»¬ä¼˜å…ˆé€‰ä¸€ä¸ªæè¿°æœ€æ¸…æ¥šçš„åˆ—ï¼Œæˆ–è€…åˆå¹¶å‡ åˆ—\n",
    "        # è®ºæ–‡ä¸­æåˆ°ä½¿ç”¨ unified description\n",
    "        mapping = {}\n",
    "        for _, row in df.iterrows():\n",
    "            uid = row['ID']\n",
    "            # è¿™é‡Œç®€å•å– Checkov Policy ä½œä¸ºæè¿°ï¼Œä½ å¯ä»¥æ ¹æ®å®é™… CSV ç»“æ„è°ƒæ•´\n",
    "            # å¦‚æœä½ æœ‰ä¸“é—¨ç”Ÿæˆçš„ 'Description' åˆ—æ›´å¥½\n",
    "            desc = (\n",
    "                str(row.get('Checkov_Policy', '')).strip() or \n",
    "                str(row.get('Kube_Linter_Policy', '')).strip() or \n",
    "                str(row.get('Terrascan_Policy', '')).strip() or \n",
    "                str(row.get('Remediation', '')).strip()\n",
    "            )\n",
    "            mapping[uid] = desc\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½ UMI æ˜ å°„ï¼Œå…± {len(mapping)} æ¡è§„åˆ™ã€‚\")\n",
    "        return mapping\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½ UMI å¤±è´¥: {e}\")\n",
    "        return {}\n",
    "\n",
    "umi_map = load_umi_mapping(UMI_CSV_PATH)\n",
    "\n",
    "# æµ‹è¯•ä¸€ä¸‹\n",
    "# print(umi_map.get(\"52\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5747f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: åŠ è½½ Mistral-7B æ¨¡å‹\n",
    "print(\"ğŸš€ æ­£åœ¨åŠ è½½ Mistral-7B-Instruct ...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½ (é€Ÿåº¦å¿«ï¼Œæ˜¾å­˜å ç”¨æä½ï¼Œçº¦ 6GB)\n",
    "# å¦‚æœä½ æƒ³è¿½æ±‚æè‡´ç²¾åº¦ï¼Œå¯ä»¥å»æ‰ load_in_4bitï¼Œæ”¹ç”¨ torch_dtype=torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,  # éœ€è¦å®‰è£… bitsandbytes\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8926d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: æ”¹è¿›åçš„ Prompt ç”Ÿæˆå‡½æ•° (åŸºäºè®ºæ–‡é™„å½•)\n",
    "\n",
    "def construct_prompt(yaml_content, error_id, error_description):\n",
    "    \"\"\"\n",
    "    æ ¹æ® GenKubeSec è®ºæ–‡å¤ç° Prompt æ¨¡æ¿ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - yaml_content: å¾…æ£€æµ‹çš„ YAML æ–‡ä»¶å†…å®¹\n",
    "    - error_id: é”™è¯¯ ID (ä¾‹å¦‚ \"LLM_error_150\" æˆ– \"150\")\n",
    "    - error_description: é”™è¯¯æè¿° (ä¾‹å¦‚ \"Indicates when a deployment uses less than three replicas\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ç¡®ä¿ error_id æ ¼å¼ä¸º LLM_error_<x>\n",
    "    # å¦‚æœä¼ å…¥çš„æ˜¯çº¯æ•°å­— \"150\"ï¼Œè‡ªåŠ¨è½¬ä¸º \"LLM_error_150\"\n",
    "    if not str(error_id).startswith(\"LLM_error_\"):\n",
    "        formatted_error_id = f\"LLM_error_{error_id}\"\n",
    "    else:\n",
    "        formatted_error_id = str(error_id)\n",
    "\n",
    "    # 2. ç³»ç»Ÿæç¤ºè¯ (System Prompt) - å®Œå…¨å¤åˆ»è®ºæ–‡å†…å®¹\n",
    "    system_instruction = f\"\"\"description: As a top DevOps engineer, you are provided with a K8s manifest that contains a misconfiguration labeled as {formatted_error_id}. Your tasks are:\n",
    "tasks:\n",
    "- Identify the line of the misconfiguration.\n",
    "- Provide the text of this line.\n",
    "- Offer reasoning for the identified misconfiguration only.\n",
    "- Suggest how to fix each identified misconfiguration.\n",
    "- Write the misconfiguration number corresponding to {formatted_error_id}.\n",
    "- Do not suggest a corrected version of the entire manifest.\n",
    "\n",
    "output format: Regardless of the input, the output must be returned in the following JSON format:\n",
    "output example:\n",
    "{{\n",
    "    \"Line number\": \"The line number where the misconfiguration occurs.\",\n",
    "    \"Line text\": \"Text of the misconfiguration as it appears in the manifest.\",\n",
    "    \"Reasoning\": \"Explanation of why this misconfiguration occurs within the manifest.\",\n",
    "    \"Remediation\": \"Description of how to fix the misconfiguration.\",\n",
    "    \"Error number\": \"{formatted_error_id}\"\n",
    "}}\"\"\"\n",
    "\n",
    "    # 3. Few-Shot Examples (Q & A)\n",
    "    # è®ºæ–‡æ–‡æœ¬ä¸­åªæœ‰ Answerï¼Œè¿™é‡Œæˆ‘æ ¹æ® Answer æ‰‹åŠ¨è¡¥å…¨äº†å¯¹åº”çš„ Input YAML (Q)ï¼Œ\n",
    "    # è¿™æ ·æ¨¡å‹æ‰èƒ½å­¦ä¼š \"çœ‹åˆ°è¿™ä¸ªè¾“å…¥ -> è¾“å‡ºè¿™ä¸ªJSON\" çš„é€»è¾‘ã€‚\n",
    "    \n",
    "    few_shot_examples = \"\"\"\n",
    "Q:\n",
    "LLM_error_134: Indicates when objects use deprecated API versions under extensions/v1beta.\n",
    "KCF:\n",
    "apiVersion: extensions/v1beta1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: test-ingress\n",
    "\n",
    "A:\n",
    "{\n",
    "    \"Line number\": 1,\n",
    "    \"Line text\": \"apiVersion: extensions/v1beta1\",\n",
    "    \"Reasoning\": \"The API version 'extensions/v1beta1' for Ingress is deprecated and not supported in newer versions of K8s. Resources should use the current API version to ensure compatibility and access to the latest features.\",\n",
    "    \"Remediation\": \"Update the apiVersion from 'extensions/v1beta1' to 'networking.k8s.io/v1' for the Ingress resource. This change ensures compatibility with newer K8s versions and leverages the latest features and fixes.\",\n",
    "    \"Error number\": \"LLM_error_153\"\n",
    "}\n",
    "\n",
    "Q:\n",
    "LLM_error_18: The default namespace should not be used\n",
    "KCF:\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: gateway-ingress\n",
    "spec:\n",
    "  rules:\n",
    "  - http:\n",
    "      paths:\n",
    "      - path: /test\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: test\n",
    "            port:\n",
    "              number: 80\n",
    "\n",
    "A:\n",
    "{\n",
    "    \"Line number\": 4,\n",
    "    \"Line text\": \"name: gateway-ingress\",\n",
    "    \"Reasoning\": \"The resource 'gateway-ingress' does not specify a namespace and therefore defaults to the 'default' namespace. Using the default namespace for production or shared environments can lead to conflicts and security issues, as it is accessible by all users in a cluster.\",\n",
    "    \"Remediation\": \"Add a namespace to the metadata section of the Ingress resource to properly isolate resources and enhance security. For example, add 'namespace: custom-namespace' below the 'name' field.\",\n",
    "    \"Error number\": \"LLM_error_9\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    # 4. å½“å‰ä»»åŠ¡ (Current Task)\n",
    "    current_task = f\"\"\"\n",
    "Q:\n",
    "{formatted_error_id}: {error_description}\n",
    "KCF:\n",
    "{yaml_content}\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "    \n",
    "    # 5. ç»„åˆæœ€ç»ˆ Prompt\n",
    "    # Mistral æ ¼å¼: [INST] System + Examples + Task [/INST]\n",
    "    # æ³¨æ„ï¼šæˆ‘ä»¬æŠŠ System Prompt å’Œ Few-Shot éƒ½æ”¾åœ¨ INST é‡Œä½œä¸ºä¸Šä¸‹æ–‡\n",
    "    full_prompt = f\"[INST] {system_instruction}\\n\\n{few_shot_examples}\\n\\n{current_task} [/INST]\"\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: è§£æä¸æ¨ç†å‡½æ•°\n",
    "\n",
    "def parse_llm_output(output_text):\n",
    "    \"\"\"\n",
    "    ä» LLM çš„å›å¤ä¸­æå– JSONã€‚\n",
    "    LLM å¯èƒ½ä¼šåœ¨ JSON å‰åè¯´åºŸè¯ï¼Œéœ€è¦æ¸…æ´—ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }\n",
    "        start = output_text.find('{')\n",
    "        end = output_text.rfind('}') + 1\n",
    "        if start != -1 and end != -1:\n",
    "            json_str = output_text[start:end]\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            return {\"error\": \"No JSON found in output\", \"raw\": output_text}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"JSON decode failed\", \"raw\": output_text}\n",
    "\n",
    "def genkube_resolve(yaml_content, detected_labels, model, tokenizer):\n",
    "    results = []\n",
    "    \n",
    "    for label in detected_labels:\n",
    "        # è§£ç æ ‡ç­¾: \"Deployment+52\" -> uid=\"52\"\n",
    "        parts = label.split('+')\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        \n",
    "        resource_kind, uid = parts\n",
    "        \n",
    "        # è·å–æè¿°\n",
    "        error_desc = umi_map.get(uid, \"Misconfiguration detected.\")\n",
    "        \n",
    "        print(f\"ğŸ” æ­£åœ¨åˆ†æé”™è¯¯: {label} (ID: {uid})...\")\n",
    "        \n",
    "        # --- å…³é”®ä¿®æ”¹ç‚¹ ---\n",
    "        # ä¼ å…¥ uid (ä¾‹å¦‚ \"52\")ï¼Œå‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨è½¬ä¸º \"LLM_error_52\"\n",
    "        prompt = construct_prompt(yaml_content, uid, error_desc)\n",
    "        # ----------------\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=512,\n",
    "                do_sample=False, \n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # è§£æ JSON\n",
    "        analysis = parse_llm_output(generated_text)\n",
    "        \n",
    "        results.append({\n",
    "            \"label\": label,\n",
    "            \"description\": error_desc,\n",
    "            \"analysis\": analysis\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7a: åŠ è½½ GenKubeDetect (æ£€æµ‹æ¨¡å‹)\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# --- é…ç½®æ£€æµ‹æ¨¡å‹è·¯å¾„ ---\n",
    "DETECT_BASE_PATH = \"/ssd_2t_1/wyq_workspace/genkubesect_structural_model\"\n",
    "DETECT_LORA_PATH = \"/ssd_2t_1/wyq_workspace/genkubesect_detection_model\"\n",
    "\n",
    "print(\"â³ æ­£åœ¨åŠ è½½æ£€æµ‹æ¨¡å‹ GenKubeDetect (CodeT5p) ...\")\n",
    "\n",
    "# 1. åŠ è½½ Tokenizer\n",
    "detect_tokenizer = AutoTokenizer.from_pretrained(DETECT_BASE_PATH, trust_remote_code=True)\n",
    "\n",
    "# 2. åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "detect_base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    DETECT_BASE_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" # è‡ªåŠ¨åˆ†é…åˆ° GPU\n",
    ")\n",
    "\n",
    "# 3. åŠ è½½ LoRA é€‚é…å™¨\n",
    "detect_model = PeftModel.from_pretrained(detect_base_model, DETECT_LORA_PATH)\n",
    "detect_model.eval()\n",
    "\n",
    "print(\"âœ… æ£€æµ‹æ¨¡å‹åŠ è½½å®Œæˆï¼ç°åœ¨æ˜¾å­˜é‡Œæœ‰ä¸¤ä¸ªæ¨¡å‹äº† (Mistral + CodeT5p)ã€‚\")\n",
    "\n",
    "def run_detection(yaml_content):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ GenKubeDetect æ¨¡å‹æ£€æµ‹ YAML\n",
    "    è¿”å›: åŸå§‹æ ‡ç­¾å­—ç¬¦ä¸² (e.g., \"Deployment+10, Service+52\")\n",
    "    \"\"\"\n",
    "    inputs = detect_tokenizer(\n",
    "        yaml_content, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512, \n",
    "        truncation=True\n",
    "    ).to(detect_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = detect_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            max_new_tokens=128,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    return detect_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7b: è¿è¡Œå®Œæ•´æµæ°´çº¿ (Detect -> Resolve)\n",
    "\n",
    "# 1. å®šä¹‰å¾…æ£€æµ‹çš„ YAML (å°±æ˜¯ä½ ä¹‹å‰æä¾›çš„é‚£ä¸ªæœ‰é—®é¢˜çš„)\n",
    "target_yaml = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nginx-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nginx\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nginx\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nginx\n",
    "        image: nginx:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸš€ é˜¶æ®µ 1: GenKubeDetect (æ£€æµ‹ä¸­...)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. è¿è¡Œæ£€æµ‹æ¨¡å‹\n",
    "raw_labels_str = run_detection(target_yaml)\n",
    "print(f\"ğŸ“¥ æ¨¡å‹åŸå§‹è¾“å‡º: {raw_labels_str}\")\n",
    "\n",
    "# 2. æ¸…æ´—æ ‡ç­¾ (String -> List)\n",
    "# ä½ çš„è¾“å‡ºç»“æœå¾ˆé•¿: \"Deployment+10, Deployment+11, ...\"\n",
    "# æˆ‘ä»¬éœ€è¦æŠŠå®ƒåˆ‡å‰²æˆåˆ—è¡¨ï¼Œå»é™¤ç©ºæ ¼\n",
    "if raw_labels_str.strip():\n",
    "    detected_labels = [label.strip() for label in raw_labels_str.split(',')]\n",
    "else:\n",
    "    detected_labels = []\n",
    "\n",
    "# å»é‡ (é˜²æ­¢æ¨¡å‹è¾“å‡ºé‡å¤æ ‡ç­¾)\n",
    "detected_labels = list(set(detected_labels))\n",
    "\n",
    "print(f\"ğŸ“‹ è§£æåˆ°çš„æ ‡ç­¾ ({len(detected_labels)} ä¸ª): {detected_labels}\")\n",
    "\n",
    "# 3. å¦‚æœæ ‡ç­¾å¤ªå¤šï¼Œä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬å¯ä»¥åªå–å‰ 3 ä¸ª\n",
    "# å¦‚æœä½ æƒ³è·‘å®Œæ‰€æœ‰ 20 ä¸ªé”™è¯¯ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™ä¸¤è¡Œæ³¨é‡Šæ‰\n",
    "if len(detected_labels) > 3:\n",
    "    print(f\"âš ï¸ é”™è¯¯å¤ªå¤šï¼Œä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œä»…å±•ç¤ºå‰ 3 ä¸ªé”™è¯¯çš„ä¿®å¤æ–¹æ¡ˆ...\")\n",
    "    detected_labels = detected_labels[:3]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ§  é˜¶æ®µ 2: GenKubeResolve (å®šä½ä¸ä¿®å¤ä¸­...)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 4. è°ƒç”¨ Mistral è¿›è¡Œè§£é‡Š (ä½¿ç”¨ä¸Šä¸€æ®µä»£ç å®šä¹‰çš„ genkube_resolve)\n",
    "# æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ ä¹‹å‰çš„ Notebook å•å…ƒæ ¼é‡Œå·²ç»å®šä¹‰äº† `genkube_resolve`, `model` (Mistral) å’Œ `tokenizer` (Mistral)\n",
    "resolution_reports = genkube_resolve(target_yaml, detected_labels, model, tokenizer)\n",
    "\n",
    "# 5. æ‰“å°æœ€ç»ˆæŠ¥å‘Š\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š æœ€ç»ˆå®‰å…¨å®¡è®¡æŠ¥å‘Š\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for res in resolution_reports:\n",
    "    print(f\"\\n[ğŸ›‘ ID: {res['label']}]\")\n",
    "    print(f\"è§„åˆ™æè¿°: {res['description']}\")\n",
    "    \n",
    "    analysis = res['analysis']\n",
    "    if \"error\" in analysis:\n",
    "        print(f\"âŒ LLM è§£æ JSON å¤±è´¥: {analysis['error']}\")\n",
    "    else:\n",
    "        print(f\"ğŸ“ è¡Œå·: {analysis.get('Line number') or analysis.get('line')}\")\n",
    "        print(f\"ğŸ’¡ åŸå› : {analysis.get('Reasoning') or analysis.get('reason')}\")\n",
    "        print(f\"ğŸ› ï¸ ä¿®å¤:\\n{analysis.get('Remediation') or analysis.get('fix')}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
