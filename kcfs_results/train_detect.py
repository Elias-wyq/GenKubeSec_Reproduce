import os
import torch
from datasets import load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# --- 1. é…ç½®è·¯å¾„ä¸å‚æ•° ---
# ä½ çš„ç»“æ„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (Base Model)
MODEL_PATH = "/ssd_2t_1/wyq_workspace/genkubesect_structural_model"
# ä½ çš„æ•°æ®é›†è·¯å¾„ (ç”± build_dataset.py ç”Ÿæˆ)
DATASET_PATH = "/home/wyq/GenKubeSec_Reproduce/kcfs_results/genkubesec_dataset"
# æœ€ç»ˆ LoRA æ¨¡å‹ä¿å­˜è·¯å¾„
OUTPUT_DIR = "/ssd_2t_1/wyq_workspace/genkubesect_detection_model"

# è¶…å‚æ•° (å‚è€ƒ GenKubeSec è®ºæ–‡)
MAX_SOURCE_LEN = 512   # CodeT5p ä¸Šé™
MAX_TARGET_LEN = 128   # æ ‡ç­¾å­—ç¬¦ä¸²é€šå¸¸ä¸é•¿
BATCH_SIZE = 8       # æ ¹æ®æ˜¾å­˜è°ƒæ•´ (4090/A100 å¯è®¾ 16-32, æ˜¾å­˜å°åˆ™ 8)
NUM_EPOCHS = 5        # å¾®è°ƒé€šå¸¸ 5-10 è½®
LEARNING_RATE = 2e-4   # LoRA å¸¸ç”¨å­¦ä¹ ç‡

def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {MODEL_PATH} ...")
    
    # --- 2. åŠ è½½ Tokenizer å’Œ æ¨¡å‹ ---
    # æ³¨æ„: CodeT5p éœ€è¦ trust_remote_code=True
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    
    # åŠ è½½ Seq2Seq æ¨¡å‹
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.float16, # ä½¿ç”¨ fp16 èŠ‚çœæ˜¾å­˜
        device_map="auto"          # è‡ªåŠ¨åˆ†é…æ˜¾å¡
    )

    # --- 3. é…ç½® LoRA (Low-Rank Adaptation) ---
    # è®ºæ–‡å‚æ•°: r=128, lora_alpha=256, dropout=0.125
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM, 
        inference_mode=False, 
        r=128, 
        lora_alpha=256, 
        lora_dropout=0.125,
        # CodeT5p (T5ç»“æ„) çš„ Attention æ¨¡å—é€šå¸¸å« 'q', 'v'
        target_modules=["q", "v"] 
    )
    
    # å°†æ¨¡å‹è½¬æ¢ä¸º PEFT æ¨¡å‹
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters() # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡ï¼Œç¡®è®¤ LoRA ç”Ÿæ•ˆ

    # --- 4. æ•°æ®é¢„å¤„ç† ---
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_PATH} ...")
    dataset = load_from_disk(DATASET_PATH)

    def preprocess_function(examples):
        # è¾“å…¥: YAML å†…å®¹
        inputs = examples["source"]
        # è¾“å‡º: é”™è¯¯æ ‡ç­¾ (å¦‚ "Deployment+10, Service+52")
        targets = examples["target"]
        
        # Tokenize è¾“å…¥
        model_inputs = tokenizer(
            inputs, 
            max_length=MAX_SOURCE_LEN, 
            padding="max_length", 
            truncation=True
        )

        # Tokenize è¾“å‡º (Labels)
        labels = tokenizer(
            targets, 
            max_length=MAX_TARGET_LEN, 
            padding="max_length", 
            truncation=True
        ).input_ids

        # å°† Padding çš„ Label ID è®¾ä¸º -100ï¼Œä»¥ä¾¿åœ¨è®¡ç®— Loss æ—¶å¿½ç•¥
        labels_with_ignore_index = []
        for label_example in labels:
            label_example = [label if label != 0 else -100 for label in label_example]
            labels_with_ignore_index.append(label_example)
        
        model_inputs["labels"] = labels_with_ignore_index
        return model_inputs

    print("âš™ï¸ æ­£åœ¨å¤„ç†æ•°æ® (Tokenization)...")
    tokenized_datasets = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=dataset["train"].column_names, # ç§»é™¤åŸå§‹åˆ—ï¼Œåªä¿ç•™ input_ids, labels
        num_proc=8
    )

    # --- 5. é…ç½®è®­ç»ƒå‚æ•° ---
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=8,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        weight_decay=0.01,

        eval_strategy="steps",
        save_strategy="steps",
        eval_steps=500,                   # æ¯ 500 æ­¥è¯„ä¼°ä¸€æ¬¡
        save_steps=500,
        save_total_limit=3,
        metric_for_best_model="eval_loss",
        # evaluation_strategy="epoch",  # æ¯ä¸ª Epoch è¯„ä¼°ä¸€æ¬¡
        # save_strategy="epoch",        # æ¯ä¸ª Epoch ä¿å­˜ä¸€æ¬¡
        # save_total_limit=2,           # åªä¿ç•™æœ€æ–°çš„ 2 ä¸ªæ¨¡å‹
        predict_with_generate=True,   # è¯„ä¼°æ—¶ç”Ÿæˆæ–‡æœ¬
        # GPU 0 æ˜¯ 3090ï¼Œå®Œç¾æ”¯æŒ bf16
        bf16=True,                        
        fp16=False,
        # fp16=True,                    # å¼€å¯æ··åˆç²¾åº¦
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_steps=100,
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸåŠ è½½éªŒè¯é›†è¡¨ç°æœ€å¥½çš„æ¨¡å‹
        report_to="none"              # ä¸ä¸Šä¼  WandB
    )

    # æ•°æ®æ•´ç†å™¨ (å¤„ç† Padding)
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model
    )

    # --- 6. å¼€å§‹è®­ç»ƒ ---
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    print("ğŸ”¥ å¼€å§‹å¾®è°ƒ (Fine-tuning)...")
    trainer.train()

    # --- 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR} ...")
    # ä¿å­˜ adapter
    model.save_pretrained(OUTPUT_DIR)
    # ä¿å­˜ tokenizer
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()