import json
import os
import random
# è®¾ç½® HF ç¼“å­˜è·¯å¾„ (ä¿æŒå’Œä½ ä¹‹å‰çš„ä¸€è‡´)
os.environ["HF_DATASETS_CACHE"] = "/ssd_2t_1/wyq_workspace/hf_cache"
from datasets import load_dataset, Dataset, DatasetDict

# --- é…ç½®è·¯å¾„ ---
# 1. ä½ çš„æ ‡ç­¾æ–‡ä»¶
LABEL_FILE = "/home/wyq/GenKubeSec_Reproduce/kcfs_results/RB_tool_results/final_labels.jsonl"
# 2. åŸå§‹æ•°æ®é›†åç§°
HF_DATASET_NAME = "substratusai/the-stack-yaml-k8s"
# 3. æœ€ç»ˆä¿å­˜çš„ Hugging Face æ ¼å¼æ•°æ®é›†è·¯å¾„
OUTPUT_DIR = "/home/wyq/GenKubeSec_Reproduce/kcfs_results/genkubesec_dataset"

def load_labels(filepath):
    """åŠ è½½æ ‡ç­¾æ–‡ä»¶ï¼Œè¿”å›å­—å…¸ {filename: labels_string}"""
    print(f"æ­£åœ¨åŠ è½½æ ‡ç­¾æ–‡ä»¶: {filepath} ...")
    label_map = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                filename = data['filename']
                labels = data['misconfig_labels']
                
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œä½œä¸ºæ¨¡å‹çš„è®­ç»ƒç›®æ ‡
                # ä¾‹å¦‚: ["Deployment+10", "Deployment+15"] -> "Deployment+10, Deployment+15"
                label_str = ", ".join(labels)
                label_map[filename] = label_str
            except json.JSONDecodeError:
                pass
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(label_map)} ä¸ªå·²æ ‡æ³¨æ–‡ä»¶ã€‚")
    return label_map

def main():
    # 1. åŠ è½½æ ‡ç­¾
    label_map = load_labels(LABEL_FILE)
    
    # 2. åŠ è½½åŸå§‹æ•°æ®é›†
    print(f"æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®é›†: {HF_DATASET_NAME} ...")
    raw_ds = load_dataset(HF_DATASET_NAME, split="train", streaming=False)
    
    # 3. æ„å»ºè®­ç»ƒæ•°æ®åˆ—è¡¨
    data_entries = []
    
    print("æ­£åœ¨åˆå¹¶ YAML å†…å®¹ä¸æ ‡ç­¾...")
    # éå†åŸå§‹æ•°æ®é›†ï¼Œæ ¹æ® file_{i}.yaml çš„è§„åˆ™è¿›è¡ŒåŒ¹é…
    for i, item in enumerate(raw_ds):
        pseudo_filename = f"file_{i}.yaml"
        
        # åªæœ‰å½“è¯¥æ–‡ä»¶æœ‰å¯¹åº”çš„é”™è¯¯æ ‡ç­¾æ—¶ï¼Œæ‰çº³å…¥è®­ç»ƒé›†
        # (GenKubeSec è®ºæ–‡ä¸»è¦å…³æ³¨æœ‰ç¼ºé™·çš„æ ·æœ¬è¿›è¡Œæ£€æµ‹è®­ç»ƒï¼Œ
        #  å¦‚æœä½ ä¹Ÿæƒ³è®©æ¨¡å‹å­¦ä¼šè¯†åˆ«â€œæ— é”™è¯¯â€æ–‡ä»¶ï¼Œå¯ä»¥ä¿ç•™ label_map ä¸­æ²¡æœ‰çš„æ–‡ä»¶å¹¶æ ‡è®°ä¸º "Safe")
        if pseudo_filename in label_map:
            content = item['content']
            target = label_map[pseudo_filename]
            
            # è¿‡æ»¤è¿‡é•¿çš„æ–‡ä»¶ (CodeT5p é™åˆ¶ 512 tokenï¼Œå¤ªé•¿çš„ YAML æ•ˆæœä¸å¥½)
            # è¿™é‡Œç®€å•ç”¨å­—ç¬¦æ•°ç²—ç•¥è¿‡æ»¤ï¼Œåç»­ Tokenizer å¤„ç†æ—¶ä¼šæˆªæ–­
            if len(content) > 10000: 
                continue

            data_entries.append({
                "source": content,   # è¾“å…¥: YAML å†…å®¹
                "target": target,    # è¾“å‡º: é”™è¯¯æ ‡ç­¾å­—ç¬¦ä¸²
                "filename": pseudo_filename
            })
            
        if (i + 1) % 50000 == 0:
            print(f"   å·²æ‰«æ {i + 1} ä¸ªåŸå§‹æ–‡ä»¶...")

    print(f"âœ… åˆå¹¶å®Œæˆã€‚æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°: {len(data_entries)}")

    # 4. åˆ›å»º Hugging Face Dataset å¯¹è±¡
    full_dataset = Dataset.from_list(data_entries)

    # 5. åˆ’åˆ†æ•°æ®é›† (80% è®­ç»ƒ, 10% éªŒè¯, 10% æµ‹è¯•)
    # é¦–å…ˆåˆ†å‡º Train å’Œ (Test + Validation)
    train_testvalid = full_dataset.train_test_split(test_size=0.2, seed=42)
    # å†å°† (Test + Validation) åˆ†ä¸º Test å’Œ Validation
    test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)

    # ç»„åˆæˆ DatasetDict
    final_dataset = DatasetDict({
        'train': train_testvalid['train'],
        'validation': test_valid['train'], # è¿™é‡Œåå­—å« train ä½†å…¶å®æ˜¯ split å‡ºæ¥çš„ä¸€åŠ
        'test': test_valid['test']
    })

    print("\næ•°æ®é›†åˆ’åˆ†è¯¦æƒ…:")
    print(f"   Train: {len(final_dataset['train'])}")
    print(f"   Validation: {len(final_dataset['validation'])}")
    print(f"   Test: {len(final_dataset['test'])}")

    # 6. ä¿å­˜åˆ°ç£ç›˜
    print(f"\næ­£åœ¨ä¿å­˜æ•°æ®é›†åˆ° {OUTPUT_DIR} ...")
    final_dataset.save_to_disk(OUTPUT_DIR)
    print("ğŸ‰ æ­å–œï¼è®­ç»ƒæ•°æ®å‡†å¤‡å°±ç»ªã€‚")

if __name__ == "__main__":
    main()