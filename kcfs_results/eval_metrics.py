import torch
from peft import PeftModel
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_from_disk
from tqdm import tqdm

# --- é…ç½® ---
BASE_MODEL = "/ssd_2t_1/wyq_workspace/genkubesect_structural_model"
LORA_MODEL = "/ssd_2t_1/wyq_workspace/genkubesect_detection_model"
DATASET_PATH = "/home/wyq/GenKubeSec_Reproduce/kcfs_results/genkubesec_dataset"
BATCH_SIZE = 16 # æ˜¾å­˜å¤Ÿå¤§å¯ä»¥å¼€å¤§

def parse_labels(label_str):
    """å°†å­—ç¬¦ä¸² 'Deployment+10, Service+52' è§£æä¸ºé›†åˆ {'Deployment+10', 'Service+52'}"""
    if not label_str or label_str.strip() == "":
        return set()
    return set([x.strip() for x in label_str.split(',')])

def calculate_metrics(predictions, references):
    total_tp = 0
    total_fp = 0
    total_fn = 0
    
    for pred_str, ref_str in zip(predictions, references):
        pred_set = parse_labels(pred_str)
        ref_set = parse_labels(ref_str)
        
        # True Positives: é¢„æµ‹å¯¹çš„
        tp = len(pred_set.intersection(ref_set))
        # False Positives: é¢„æµ‹äº†ä½†å®é™…æ²¡æœ‰çš„ (è¯¯æŠ¥)
        fp = len(pred_set - ref_set)
        # False Negatives: å®é™…æœ‰ä½†æ²¡é¢„æµ‹å‡ºæ¥çš„ (æ¼æŠ¥)
        fn = len(ref_set - pred_set)
        
        total_tp += tp
        total_fp += fp
        total_fn += fn
        
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return precision, recall, f1

def main():
    # 1. åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    base_model = AutoModelForSeq2SeqLM.from_pretrained(
        BASE_MODEL, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto"
    )
    model = PeftModel.from_pretrained(base_model, LORA_MODEL)
    model.eval()

    # 2. åŠ è½½æµ‹è¯•é›†
    print("æ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
    dataset = load_from_disk(DATASET_PATH)
    test_data = dataset["test"] # åªä½¿ç”¨æµ‹è¯•é›†
    
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
    
    # 3. æ‰¹é‡æ¨ç†
    predictions = []
    references = test_data["target"] # çœŸå®æ ‡ç­¾
    inputs = test_data["source"]     # è¾“å…¥ YAML

    print("å¼€å§‹æ¨ç†è¯„ä¼°...")
    # æ‰‹åŠ¨ Batch å¤„ç†
    for i in tqdm(range(0, len(inputs), BATCH_SIZE)):
        batch_inputs = inputs[i : i + BATCH_SIZE]
        
        # Tokenize
        model_inputs = tokenizer(
            batch_inputs, 
            max_length=512, 
            padding=True, 
            truncation=True, 
            return_tensors="pt"
        ).to(model.device)
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                input_ids=model_inputs["input_ids"],
                attention_mask=model_inputs["attention_mask"],
                max_new_tokens=128,
                num_beams=3 # ç¨å¾®é™ä½ beam åŠ é€Ÿè¯„ä¼°
            )
        
        # Decode
        batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        predictions.extend(batch_preds)

    # 4. è®¡ç®—æŒ‡æ ‡
    precision, recall, f1 = calculate_metrics(predictions, references)

    print("\n" + "="*30)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ (Test Set)")
    print("="*30)
    print(f"Precision (ç²¾ç¡®ç‡): {precision:.4f}")
    print(f"Recall    (å¬å›ç‡): {recall:.4f}")
    print(f"F1 Score  (ç»¼åˆåˆ†): {f1:.4f}")
    print("="*30)

if __name__ == "__main__":
    main()