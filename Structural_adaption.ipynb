{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9441a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch transformers datasets scikit-learn accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®å®Œæˆã€‚æ•°æ®ç›®å½•: /home/wyq/kcfs/raw_100_yaml_files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# ================= é…ç½®å‚æ•° =================\n",
    "# è®ºæ–‡é€‰ç”¨çš„åŸºåº§æ¨¡å‹ï¼šCodeT5p 770M (Encoder-Decoder) \n",
    "MODEL_NAME = \"Salesforce/codet5p-770m\"\n",
    "\n",
    "# æ‚¨çš„æ•°æ®è·¯å¾„\n",
    "DATA_DIR = \"/home/wyq/GenKubeSec_Reproduce/raw_100_yaml_files\"\n",
    "\n",
    "# è®ºæ–‡é™åˆ¶ï¼šCodeT5p 770M çš„æœ€å¤§ Token ä¸º 512 \n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# è®ºæ–‡è®¾å®šï¼šMasking æ¯”ä¾‹ä¸º 15% \n",
    "MASK_RATIO = 0.15\n",
    "\n",
    "# è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"./genkubesect_structural_model\"\n",
    "\n",
    "# è®­ç»ƒå‚æ•° (æ ¹æ®æ‚¨çš„æ˜¾å­˜è°ƒæ•´ï¼ŒRTX 4090 å¯ä»¥å°è¯• batch_size=4 æˆ– 8)\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "SEED = 42\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥å¤ç°ç»“æœ\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"é…ç½®å®Œæˆã€‚æ•°æ®ç›®å½•: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595f7996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æ‰«æç›®å½•: /home/wyq/kcfs/raw_100_yaml_files\n",
      "æ‰¾åˆ° 100 ä¸ª YAML æ–‡ä»¶\n",
      "æˆåŠŸåŠ è½½ 100 ä¸ªæœ‰æ•ˆæ–‡ä»¶å†…å®¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "def load_yaml_files(directory):\n",
    "    \"\"\"\n",
    "    åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰çš„ .yaml å’Œ .yml æ–‡ä»¶ä½œä¸ºæœªæ ‡æ³¨æ•°æ®é›† [cite: 159, 214]ã€‚\n",
    "    \"\"\"\n",
    "    files = glob.glob(os.path.join(directory, \"**/*.yaml\"), recursive=True)\n",
    "    files += glob.glob(os.path.join(directory, \"**/*.yml\"), recursive=True)\n",
    "    \n",
    "    print(f\"æ­£åœ¨æ‰«æç›®å½•: {directory}\")\n",
    "    print(f\"æ‰¾åˆ° {len(files)} ä¸ª YAML æ–‡ä»¶\")\n",
    "    \n",
    "    contents = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            with open(f, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                if content.strip(): # è¿‡æ»¤ç©ºæ–‡ä»¶\n",
    "                    contents.append(content)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error reading {f}: {e}\") # è°ƒè¯•æ—¶å¯æ‰“å¼€\n",
    "            pass\n",
    "            \n",
    "    return contents\n",
    "\n",
    "# æ‰§è¡ŒåŠ è½½\n",
    "raw_texts = load_yaml_files(DATA_DIR)\n",
    "print(f\"æˆåŠŸåŠ è½½ {len(raw_texts)} ä¸ªæœ‰æ•ˆæ–‡ä»¶å†…å®¹ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c338a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beae317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æŸ¥ç½‘ç»œè¿é€šæ€§ï¼šhttps://huggingface.co (timeout=5s) ...\n",
      "ç½‘ç»œå¯è¾¾ï¼ŒçŠ¶æ€ç ï¼š200\n",
      "å¼€å§‹åŠ è½½ tokenizer: Salesforce/codet5p-770m\n",
      "tokenizer åŠ è½½å®Œæˆï¼Œè€—æ—¶: 2.3s\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:36217\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:36217\"\n",
    "\n",
    "# å¯é€‰ï¼šåœ¨åŠ è½½å¤§æ¨¡å‹å‰åšä¸€ä¸ªå¿«é€Ÿçš„ç½‘ç»œè¿é€šæ€§æ£€æŸ¥\n",
    "import time\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    requests = None\n",
    "\n",
    "hf_check_url = \"https://huggingface.co\"\n",
    "if requests is not None:\n",
    "    try:\n",
    "        print(f\"æ£€æŸ¥ç½‘ç»œè¿é€šæ€§ï¼š{hf_check_url} (timeout=5s) ...\")\n",
    "        r = requests.head(hf_check_url, timeout=5)\n",
    "        print(f\"ç½‘ç»œå¯è¾¾ï¼ŒçŠ¶æ€ç ï¼š{r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(\"æ— æ³•è®¿é—® huggingface.coï¼ˆå¯èƒ½ç½‘ç»œæˆ–ä»£ç†é—®é¢˜ï¼‰ã€‚å°è¯•ç»§ç»­ï¼Œä½†åŠ è½½å¯èƒ½ä¼šé˜»å¡æˆ–å¤±è´¥ï¼š\", e)\n",
    "else:\n",
    "    print(\"æœªæ£€æµ‹åˆ° requests åº“ï¼Œè·³è¿‡ç½‘ç»œè¿é€šæ€§å¿«é€Ÿæ£€æŸ¥ã€‚\")\n",
    "\n",
    "# åˆå§‹åŒ– Tokenizerï¼ˆè®°å½•åŠ è½½æ—¶é—´ä»¥ä¾¿åˆ¤æ–­æ˜¯å¦å¡ä½ï¼‰\n",
    "print(f\"å¼€å§‹åŠ è½½ tokenizer: {MODEL_NAME}\")\n",
    "t0 = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "t1 = time.time()\n",
    "print(f\"tokenizer åŠ è½½å®Œæˆï¼Œè€—æ—¶: {t1 - t0:.1f}s\")\n",
    "\n",
    "# ç¡®ä¿æœ‰ Mask Token (CodeT5 é»˜è®¤å¯èƒ½æœ‰ç‰¹å®š sentinel tokensï¼Œè¿™é‡Œä¸ºäº†ç®€åŒ–ä½¿ç”¨é€šç”¨ mask ç­–ç•¥)\n",
    "if not tokenizer.mask_token:\n",
    "    tokenizer.mask_token = \"<mask>\"\n",
    "\n",
    "def prepare_structural_tasks(examples):\n",
    "    \"\"\"\n",
    "    æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼šå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„ Input-Target å¯¹ã€‚\n",
    "    åŒ…å«æ•°æ®è¿‡æ»¤é€»è¾‘ï¼šä¸¢å¼ƒè¶…è¿‡ 512 Tokens çš„æ ·æœ¬ [cite: 278, 400]ã€‚\n",
    "    åœ¨æ­¤å‡½æ•°å†…åŠ å…¥è¿›åº¦æ˜¾ç¤ºï¼ˆä½¿ç”¨ tqdmï¼‰å¹¶ç»Ÿè®¡è¢«ä¸¢å¼ƒçš„æ ·æœ¬æ•°ã€‚\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    skipped = 0\n",
    "    total = 0\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except Exception:\n",
    "        tqdm = None\n",
    "\n",
    "    iterator = examples['text'] if tqdm is None else tqdm(examples['text'], desc=\"preprocess\")\n",
    "    for content in iterator:\n",
    "        total += 1\n",
    "        # 1. åˆå§‹ Tokenize (ä¸æˆªæ–­ï¼Œç”¨äºæ£€æŸ¥é•¿åº¦)\n",
    "        try:\n",
    "            tokens = tokenizer(content, truncation=False, add_special_tokens=False)['input_ids']\n",
    "        except Exception as e:\n",
    "            # å¦‚æœ tokenizer å› å¼‚å¸¸ä¸­æ–­ï¼Œæ‰“å°ä¿¡æ¯å¹¶è·³è¿‡è¯¥æ ·æœ¬\n",
    "            print(\"Tokenizer å¤„ç†æŸæ¡æ ·æœ¬æ—¶å‡ºç°å¼‚å¸¸ï¼Œå·²è·³è¿‡ï¼š\", e)\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # 2. è¿‡æ»¤é€»è¾‘ï¼šä¸¢å¼ƒè¶…è¿‡ 512 token çš„é•¿æ–‡ä»¶ [cite: 279]\n",
    "        if len(tokens) > MAX_LENGTH - 5:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # 3. éšæœºé€‰æ‹©ä»»åŠ¡ç±»å‹ (50% æ¦‚ç‡)\n",
    "        task_type = random.choice(['NSP', 'MASKING'])\n",
    "\n",
    "        if task_type == 'NSP':\n",
    "            # --- Next Sentence Prediction (NSP)  ---\n",
    "            # é¢„æµ‹ååŠéƒ¨åˆ†åŸºäºå‰åŠéƒ¨åˆ†\n",
    "            split_point = len(tokens) // 2\n",
    "            input_ids = tokens[:split_point]\n",
    "            label_ids = tokens[split_point:]\n",
    "            \n",
    "            inputs.append(tokenizer.decode(input_ids))\n",
    "            targets.append(tokenizer.decode(label_ids))\n",
    "        else:\n",
    "            # --- Masking  ---\n",
    "            # éšæœºé®è”½ 15% çš„ Token\n",
    "            num_mask = max(1, int(len(tokens) * MASK_RATIO))\n",
    "            mask_indices = set(random.sample(range(len(tokens)), num_mask))\n",
    "            \n",
    "            masked_input_ids = []\n",
    "            for idx, token_id in enumerate(tokens):\n",
    "                if idx in mask_indices:\n",
    "                    masked_input_ids.append(tokenizer.mask_token_id)\n",
    "                else:\n",
    "                    masked_input_ids.append(token_id)\n",
    "            \n",
    "            # ç›®æ ‡æ˜¯æ¢å¤åŸå§‹æ–‡æœ¬ (Denoising)\n",
    "            inputs.append(tokenizer.decode(masked_input_ids))\n",
    "            targets.append(content)\n",
    "\n",
    "    # 4. æœ€ç»ˆç¼–ç \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # å°† Pad Token çš„ label è®¾ä¸º -100ï¼Œè®¡ç®— loss æ—¶å¿½ç•¥\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    print(f\"prepare_structural_tasks: å¤„ç†æ€»æ•°={total}, è·³è¿‡={skipped}, æœ‰æ•ˆ={len(inputs)}\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce91dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åº”ç”¨é¢„å¤„ç† (Masking & NSP)... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5cc29b419148718e832150eb1c0e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 139.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=2, æœ‰æ•ˆ=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 78.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=2, æœ‰æ•ˆ=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 831.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=2, æœ‰æ•ˆ=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 820.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=1, æœ‰æ•ˆ=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1016.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=1, æœ‰æ•ˆ=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1006.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=1, æœ‰æ•ˆ=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1290.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=0, æœ‰æ•ˆ=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1447.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=1, æœ‰æ•ˆ=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1257.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=1, æœ‰æ•ˆ=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 836.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_structural_tasks: å¤„ç†æ€»æ•°=10, è·³è¿‡=3, æœ‰æ•ˆ=7\n",
      "é¢„å¤„ç†å®Œæˆã€‚\n",
      "åŸå§‹æ–‡ä»¶æ•°: 100\n",
      "ç¬¦åˆé•¿åº¦è¦æ±‚(<=512 tokens)çš„æ ·æœ¬æ•°: 86\n",
      "è®­ç»ƒé›†å¤§å°: 77\n",
      "éªŒè¯é›†å¤§å°: 9\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºåŸå§‹æ•°æ®é›†\n",
    "raw_dataset = Dataset.from_dict({\"text\": raw_texts})\n",
    "\n",
    "print(\"å¼€å§‹åº”ç”¨é¢„å¤„ç† (Masking & NSP)... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚\")\n",
    "processed_dataset = raw_dataset.map(\n",
    "    prepare_structural_tasks,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Processing dataset\"\n",
    ")\n",
    "\n",
    "print(f\"é¢„å¤„ç†å®Œæˆã€‚\")\n",
    "print(f\"åŸå§‹æ–‡ä»¶æ•°: {len(raw_dataset)}\")\n",
    "print(f\"ç¬¦åˆé•¿åº¦è¦æ±‚(<=512 tokens)çš„æ ·æœ¬æ•°: {len(processed_dataset)}\")\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (90% è®­ç»ƒ, 10% éªŒè¯) [cite: 216]\n",
    "# è™½ç„¶è®ºæ–‡æ˜¯ 80/10/10ï¼Œä½†åœ¨ Structural é˜¶æ®µæˆ‘ä»¬å¯ä»¥å¤šç”¨ä¸€ç‚¹æ•°æ®åšè®­ç»ƒ\n",
    "split_dataset = processed_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"éªŒè¯é›†å¤§å°: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b5c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŸ¥çœ‹è®­ç»ƒé›†æ ·æœ¬:\n",
      "\n",
      "==================== æ ·æœ¬ 1 (Index: 38) ====================\n",
      "ğŸ”´ [Input (æ¨¡å‹è¾“å…¥)]:\n",
      "<s>apiVersion: v1\n",
      "data: {}\n",
      "kind: Secret\n",
      "metadata:\n",
      "  labels:\n",
      "    app.kubernetes.io/component: query-layer\n",
      "    app.kubernetes.io/instance: thanos-querier\n",
      "    app.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><...\n",
      "\n",
      "ğŸŸ¢ [Label (é¢„æµ‹ç›®æ ‡)]:\n",
      "<s>kubernetes.io/name: thanos-query\n",
      "    app.kubernetes.io/version: 0.19.0\n",
      "  name: thanos-querier-oauth-cookie\n",
      "  namespace: openshift-monitoring\n",
      "type: Opaque\n",
      "</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad...\n",
      "\n",
      "ğŸ” ä»»åŠ¡ç±»å‹æ¨æ–­: NSP (ä¸‹ä¸€å¥é¢„æµ‹ / åºåˆ—è¡¥å…¨)\n",
      "\n",
      "==================== æ ·æœ¬ 2 (Index: 63) ====================\n",
      "ğŸ”´ [Input (æ¨¡å‹è¾“å…¥)]:\n",
      "<s>apiVersion:<mask>1\n",
      "kind: Secret\n",
      "metadata:\n",
      "  name: cloudfl<mask>-credentials\n",
      "  namespace: default<mask><mask> Op<mask>\n",
      "data:<mask> # replace '...' with<mask><mask> as base64\n",
      "  # The token should have permission of<mask>:Read and DNS:Edit for all<mask>.\n",
      "  #<mask> can<mask> exclude certain<mask>\n",
      "  #<mask><mask>://support.cloudflare.com/hc/en-us/articles<mask>200167836-Managing-API-Tokens-and-Keys<mask> # For details see https://github.com/gardener/external-dns-management/blob/<mask>/doc/<mask>fl...\n",
      "\n",
      "ğŸŸ¢ [Label (é¢„æµ‹ç›®æ ‡)]:\n",
      "<s>apiVersion: v1\n",
      "kind: Secret\n",
      "metadata:\n",
      "  name: cloudflare-credentials\n",
      "  namespace: default\n",
      "type: Opaque\n",
      "data:\n",
      "  # replace '...' with values encoded as base64\n",
      "  # The token should have permission of Zone:Read and DNS:Edit for all zones.\n",
      "  # You can optionally exclude certain zones\n",
      "  # see https://support.cloudflare.com/hc/en-us/articles/200167836-Managing-API-Tokens-and-Keys\n",
      "  # For details see https://github.com/gardener/external-dns-management/blob/master/doc/cloudflare/README.md#using-the-ap...\n",
      "\n",
      "ğŸ” ä»»åŠ¡ç±»å‹æ¨æ–­: Masking (å¡«ç©ºä»»åŠ¡)\n",
      "\n",
      "==================== æ ·æœ¬ 3 (Index: 14) ====================\n",
      "ğŸ”´ [Input (æ¨¡å‹è¾“å…¥)]:\n",
      "<s>apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: four-global\n",
      "data:\n",
      "  URL_LALA_HOST: http</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>...\n",
      "\n",
      "ğŸŸ¢ [Label (é¢„æµ‹ç›®æ ‡)]:\n",
      "<s>://lala:8080/omni-bomni\n",
      "  URL_MOSTEST: https://ghost/omen/mostests\n",
      "</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><...\n",
      "\n",
      "ğŸ” ä»»åŠ¡ç±»å‹æ¨æ–­: NSP (ä¸‹ä¸€å¥é¢„æµ‹ / åºåˆ—è¡¥å…¨)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def show_random_sample(dataset, tokenizer, num_samples=3):\n",
    "    \"\"\"\n",
    "    éšæœºæŠ½å–æ ·æœ¬å¹¶è§£ç æ˜¾ç¤ºï¼Œç”¨äºæ£€æŸ¥é¢„å¤„ç†é€»è¾‘ã€‚\n",
    "    \"\"\"\n",
    "    # ç¡®ä¿æ ·æœ¬æ•°ä¸è¶…è¿‡æ•°æ®é›†å¤§å°\n",
    "    num_samples = min(num_samples, len(dataset))\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        print(f\"\\n{'='*20} æ ·æœ¬ {i+1} (Index: {idx}) {'='*20}\")\n",
    "        \n",
    "        # 1. è§£ç  Input\n",
    "        # skip_special_tokens=False æ˜¯ä¸ºäº†èƒ½çœ‹åˆ° padding å’Œ mask token\n",
    "        input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
    "        \n",
    "        # 2. è§£ç  Label\n",
    "        # æ³¨æ„ï¼šæˆ‘ä»¬åœ¨é¢„å¤„ç†æ—¶å°† padding çš„ label è®¾ä¸ºäº† -100ï¼Œ\n",
    "        # -100 åœ¨ tokenizer ä¸­æ— æ³•è§£ç ï¼Œå¿…é¡»å…ˆæ›¿æ¢å› pad_token_id\n",
    "        label_ids = [\n",
    "            (token if token != -100 else tokenizer.pad_token_id) \n",
    "            for token in sample['labels']\n",
    "        ]\n",
    "        label_text = tokenizer.decode(label_ids, skip_special_tokens=False)\n",
    "        \n",
    "        # 3. æ‰“å°å¯¹æ¯”\n",
    "        print(f\"ğŸ”´ [Input (æ¨¡å‹è¾“å…¥)]:\\n{input_text[:500]}...\") # åªæ‰“å°å‰500å­—ç¬¦é¿å…åˆ·å±\n",
    "        print(f\"\\nğŸŸ¢ [Label (é¢„æµ‹ç›®æ ‡)]:\\n{label_text[:500]}...\")\n",
    "        \n",
    "        # 4. ç®€å•åˆ†æä»»åŠ¡ç±»å‹\n",
    "        if \"<mask>\" in input_text or tokenizer.mask_token in input_text:\n",
    "            print(\"\\nğŸ” ä»»åŠ¡ç±»å‹æ¨æ–­: Masking (å¡«ç©ºä»»åŠ¡)\")\n",
    "        else:\n",
    "            print(\"\\nğŸ” ä»»åŠ¡ç±»å‹æ¨æ–­: NSP (ä¸‹ä¸€å¥é¢„æµ‹ / åºåˆ—è¡¥å…¨)\")\n",
    "\n",
    "# ä»è®­ç»ƒé›†ä¸­æŸ¥çœ‹\n",
    "print(\"æŸ¥çœ‹è®­ç»ƒé›†æ ·æœ¬:\")\n",
    "show_random_sample(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1184cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ¨¡å‹: Salesforce/codet5p-770m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde8e6327fe54f6e891d62c0c96da335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/770 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1226737cd908463d8b4847073986e97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å·²åŠ è½½è‡³: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"åŠ è½½æ¨¡å‹: {MODEL_NAME}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ç§»åŠ¨åˆ° GPU (å¦‚æœå¯ç”¨)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"æ¨¡å‹å·²åŠ è½½è‡³: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fca83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47387/1993131901.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # æ‰¹æ¬¡å¤§å°è®¾ç½®\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    # ä¼˜åŒ–å‚æ•°\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # ä¿å­˜ä¸è¯„ä¼°ç­–ç•¥\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,  # åªä¿ç•™æœ€è¿‘2ä¸ªcheckpointï¼ŒèŠ‚çœç©ºé—´\n",
    "    \n",
    "    # ç²¾åº¦ä¼˜åŒ–\n",
    "    fp16=torch.cuda.is_available(), # å¦‚æœæœ‰ GPU æ¨èå¼€å¯æ··åˆç²¾åº¦\n",
    "    \n",
    "    # æ—¥å¿—\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # å…¶ä»–\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\" # ä¸ä¸Šä¼ åˆ° wandb ç­‰å¹³å°ï¼Œä»…æœ¬åœ°\n",
    ")\n",
    "\n",
    "# æ•°æ®æ•´ç†å™¨\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# åˆå§‹åŒ– Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d00e0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹ Structural Adaptation è®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/parallel/data_parallel.py:38: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 2 on device 2.\nOriginal Traceback (most recent call last):\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/parallel/parallel_apply.py\", line 99, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 1727, in forward\n    encoder_outputs = self.encoder(\n        input_ids=input_ids,\n    ...<5 lines>...\n        return_dict=return_dict,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 1100, in forward\n    layer_outputs = layer_module(\n        hidden_states,\n    ...<11 lines>...\n        cache_position=cache_position,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 687, in forward\n    self_attention_outputs = self.layer[0](\n        hidden_states,\n    ...<6 lines>...\n        cache_position=cache_position,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 603, in forward\n    attention_output = self.SelfAttention(\n        normed_hidden_states,\n    ...<6 lines>...\n        cache_position=cache_position,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 529, in forward\n    scores = torch.matmul(query_states, key_states.transpose(3, 2))\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 2 has a total capacity of 10.57 GiB of which 17.19 MiB is free. Including non-PyTorch memory, this process has 10.54 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 678.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33må¼€å§‹ Structural Adaptation è®­ç»ƒ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå–å†³äºæ‚¨çš„ GPU æ€§èƒ½å’Œæ•°æ®é‡\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/parallel/parallel_apply.py:129\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n\u001b[32m    127\u001b[39m     output = results[i]\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     outputs.append(output)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: Caught OutOfMemoryError in replica 2 on device 2.\nOriginal Traceback (most recent call last):\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/parallel/parallel_apply.py\", line 99, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 1727, in forward\n    encoder_outputs = self.encoder(\n        input_ids=input_ids,\n    ...<5 lines>...\n        return_dict=return_dict,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 1100, in forward\n    layer_outputs = layer_module(\n        hidden_states,\n    ...<11 lines>...\n        cache_position=cache_position,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 687, in forward\n    self_attention_outputs = self.layer[0](\n        hidden_states,\n    ...<6 lines>...\n        cache_position=cache_position,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 603, in forward\n    attention_output = self.SelfAttention(\n        normed_hidden_states,\n    ...<6 lines>...\n        cache_position=cache_position,\n    )\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wyq/.conda/envs/genkubesect/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py\", line 529, in forward\n    scores = torch.matmul(query_states, key_states.transpose(3, 2))\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 2 has a total capacity of 10.57 GiB of which 17.19 MiB is free. Including non-PyTorch memory, this process has 10.54 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 678.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "print(\"å¼€å§‹ Structural Adaptation è®­ç»ƒ...\")\n",
    "# è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå–å†³äºæ‚¨çš„ GPU æ€§èƒ½å’Œæ•°æ®é‡\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a168d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(trainer):\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    # æå–è®­ç»ƒé›† loss\n",
    "    train_steps = []\n",
    "    train_loss = []\n",
    "    \n",
    "    # æå–éªŒè¯é›† loss\n",
    "    eval_steps = []\n",
    "    eval_loss = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if \"loss\" in log and \"epoch\" in log:\n",
    "            train_steps.append(log[\"epoch\"])\n",
    "            train_loss.append(log[\"loss\"])\n",
    "        if \"eval_loss\" in log and \"epoch\" in log:\n",
    "            eval_steps.append(log[\"epoch\"])\n",
    "            eval_loss.append(log[\"eval_loss\"])\n",
    "            \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_steps, train_loss, label=\"Training Loss\")\n",
    "    if eval_loss:\n",
    "        plt.plot(eval_steps, eval_loss, label=\"Validation Loss\", linestyle='--')\n",
    "        \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42242ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Phase 2 - Step 1 (Structural Adaptation) å®Œæˆï¼\")\n",
    "print(\"æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œä¸‹ä¸€é˜¶æ®µï¼šä½¿ç”¨æ ‡æ³¨æ•°æ®è¿›è¡Œ LoRA å¾®è°ƒ (Misconfig Detection)ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genkubesect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
